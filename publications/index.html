<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Yadong Xie</title>
    <meta name="author" content="Yadong  Xie">
    <meta name="description" content="Publications by categories in reversed chronological order.">
    <!-- OpenGraph -->
    <meta property="og:site_name" content="Yadong Xie">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Yadong Xie | Publications">
    <meta property="og:url" content="https://yd-xie.github.io/publications/">
    <meta property="og:description" content="Publications by categories in reversed chronological order.">
    
    <meta property="og:locale" content="en">

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Publications">
    <meta name="twitter:description" content="Publications by categories in reversed chronological order.">
    
    

    <!-- Schema.org -->
    <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Yadong  Xie"
        },
        "url": "https://yd-xie.github.io/publications/",
        "@type": "WebSite",
        "description": "Publications by categories in reversed chronological order.",
        "headline": "Publications",
        "sameAs": ["https://orcid.org/0000-0002-2467-4240", "https://scholar.google.com/citations?user=bi8AlbMAAAAJ", "https://www.researchgate.net/profile/Yadong-Xie-2"],
        "name": "Yadong  Xie",
        "@context": "https://schema.org"
    }
    </script>


    <!-- Bootstrap & MDB -->
    <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
    <!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light">

    

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://yd-xie.github.io/publications/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Yadong Xie</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/news/">News</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">Awards</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fa-solid fa-moon"></i>
                  <i class="fa-solid fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
<div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">Publications by categories in reversed chronological order.</p>
  </header>

  <article>
    <!-- _pages/publications.md -->
<div class="publications">

<h2 class="bibliography">2025</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">SIGCOMM</abbr></div>
<!-- Entry bib key -->
        <div id="Sigcomm_acoustic" class="col-sm-8">
        <!-- Title -->
        <div class="title">Acoustic Backscatter Network for Vehicle Body-in-White</div>
        <!-- Author -->
        <div class="author">
        

        Weiguo Wang,¬†Yuan He,¬†<b><em>Yadong Xie</em></b>,¬†Chuyue Xie,¬†Yi Kai,¬†and¬†Chengchen Hu</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2025 ACM Special Interest Group on Data Communication (SIGCOMM)</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a novel approach to monitor the Body in White (BiW), the fundamental metallic structure of a vehicle. Existing monitoring methods, including both wired and wireless sensor systems, face significant challenges due to integration complexity, weight considerations, material costs, and signal blockage within the metallic environment. To overcome these limitations, we introduce Arach-Net, an acoustic backscatter network that leverages the conductive properties of the BiW to propagate vibration signals for energy transfer and data communication. This system comprises batteryfree tags that harvest energy from BiW vibrations and utilize a backscatter technique for efficient communication, thereby eliminating the need for external power sources and reducing the power consumption. We address key challenges such as power sufficiency for tag activation and sustained operation, and collision reduction in network communication, by designing an ultra-low power backscatter tag and a distributed slot allocation protocol. We implement ArachNet, and deploy 12 tags onto the BiW of an electric SUV car. The evaluation results show that the power consumption of the tag is 51.0 ùúáW for uplink packet transmission, and 24.8 ùúáW for downlink packet reception. With our network protocol, the slot utilization can be up to 81.2%.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sigcomm_acoustic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Acoustic Backscatter Network for Vehicle Body-in-White}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Weiguo and He, Yuan and Xie, Yadong and Xie, Chuyue and Kai, Yi and Hu, Chengchen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 ACM Special Interest Group on Data Communication (SIGCOMM)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">WASA</abbr></div>
<!-- Entry bib key -->
        <div id="10.1007/978-981-96-8728-2_32" class="col-sm-8">
        <!-- Title -->
        <div class="title">MASS: Empowering Wi-Fi Human Sensing with¬†Metasurface-Assisted Sample Synthesis</div>
        <!-- Author -->
        <div class="author">
        

        Jiaming Gu,¬†Shaonan Chen,¬†Yimiao Sun,¬†<b><em>Yadong Xie</em></b>,¬†Rui Xi,¬†Qiang Cheng,¬†and¬†Yuan He</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Wireless Artificial Intelligent Computing Systems and Applications (WASA)</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://link.springer.com/chapter/10.1007/978-981-96-8728-2_32" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Wi-Fi human sensing has attracted numerous research studies over the past decade.¬†The rapid advancement of machine learning technology further boosts the development of Wi-Fi human sensing. However, current Wi-Fi human sensing suffers from the ‚Äúdata scarcity‚Äù problem: all the existing proposals require collecting a large amount of human-based datasets¬†to train the sensing models, which is labor-intensive and may raise ethical concerns¬†in certain scenarios. This obstacle seriously restricts the size, quality, and diversity¬†of available datasets, thereby affecting the sensing performance in terms of accuracy¬†and cross-domain applicability. In order to solve this problem, we in this paper propose Metasurface-Assisted Sample Synthesis (MASS), a novel approach to synthesize high-fidelity Wi-Fi sensing samples that effectively capture both the essential features of human motion and environment-specific multipath characteristics without requiring human involvement. The evaluation results show that MASS is effective in boosting the machine learning performance, improving the classification accuracy by 18%, and enhancing the cross-domain sensing accuracy¬†by 22%. These findings underscore the potential of MASS to facilitate the creation¬†of high-quality, diverse datasets with minimal human involvement and associated labor costs.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1007/978-981-96-8728-2_32</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Jiaming and Chen, Shaonan and Sun, Yimiao and Xie, Yadong and Xi, Rui and Cheng, Qiang and He, Yuan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MASS: Empowering Wi-Fi Human Sensing with¬†Metasurface-Assisted Sample Synthesis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Wireless Artificial Intelligent Computing Systems and Applications (WASA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{394--405}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-981-96-8728-2_32}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">ICPADS</abbr></div>
<!-- Entry bib key -->
        <div id="10763597" class="col-sm-8">
        <!-- Title -->
        <div class="title">mmJaw: Remote Jaw Gesture Recognition with COTS mmWave Radar</div>
        <!-- Author -->
        <div class="author">
        

        Awais Ahmad Siddiqi,¬†Yuan He,¬†Yande Chen,¬†Yimao Sun,¬†Shufan Wang,¬†and¬†<b><em>Yadong Xie</em></b>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10763597" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the increasing prevalence of IoT devices and smart systems in daily life, there is a growing demand for new modalities in Human-Computer Interaction (HCI) to improve accessibility, particularly for users who require hands-free and eyes-free interaction in contexts like VR environments, as well as for individuals with special needs or limited mobility. In this paper, we propose teeth gestures as an input modality for HCI. We find that teeth gestures, such as tapping, clenching, and sliding, are generated by various facial muscle movements that are often imperceptible to the naked eye but can be effectively captured using mm-wave radar. By capturing and analyzing the distinct patterns of these muscle movements, we propose a hands-free and eyes-free HCI solution based on three different gestures. Key challenges addressed in this paper include user range identification amidst background noise and other irrelevant facial movements. Results from 16 volunteers demonstrate the robustness of our approach, achieving 93% accuracy for up to a 2.5m range.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10763597</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Siddiqi, Awais Ahmad and He, Yuan and Chen, Yande and Sun, Yimao and Wang, Shufan and Xie, Yadong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{mmJaw: Remote Jaw Gesture Recognition with COTS mmWave Radar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{52-59}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human computer interaction;Accuracy;Radar detection;Radar;Muscles;Smart systems;Robustness;Sensors;Internet of Things;Millimeter wave communication;mmWave;Sensing;Human-Computer Interface;Teeth Gestures}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICPADS63350.2024.00017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">ICPADS</abbr></div>
<!-- Entry bib key -->
        <div id="10763548" class="col-sm-8">
        <!-- Title -->
        <div class="title">mmHRR: Monitoring Heart Rate Recovery with Millimeter Wave Radar</div>
        <!-- Author -->
        <div class="author">
        

        Ziheng Mao,¬†Yuan He,¬†Jia Zhang,¬†Yimiao Sun,¬†<b><em>Yadong Xie</em></b>,¬†and¬†Xiuzhen Guo</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10763548" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Heart rate recovery (HRR) within the initial minute following exercise is a widely utilized metric for assessing cardiac autonomic function in individuals and predicting mortality risk in patients with cardiovascular disease. However, prevailing solutions for HRR monitoring typically involve the use of specialized medical equipment or contact wearable sensors, resulting in high costs and poor user experience. In this paper, we propose a contactless HRR monitoring technique, mmHRR, which achieves accurate heart rate (HR) estimation with a commercial mmWave radar. Unlike HR estimation at rest, the HR varies quickly after exercise and the heartbeat signal entangles with the respiration harmonics. To overcome these hurdles and effectively estimate the HR from the weak and non-stationary heartbeat signal, we propose a novel signal processing pipeline, including dynamic target tracking, adaptive heartbeat signal extraction, and accurate HR estimation with composite sliding windows. Real-world experiments demonstrate that mmHRR exhibits exceptional robustness across diverse environmental conditions, and achieves an average HR estimation error of 3.31 bpm (beats per minute), 71% lower than that of the state-of-the-art method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10763548</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Ziheng and He, Yuan and Zhang, Jia and Sun, Yimiao and Xie, Yadong and Guo, Xiuzhen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{mmHRR: Monitoring Heart Rate Recovery with Millimeter Wave Radar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Accuracy;Target tracking;Heart beat;Signal processing algorithms;Millimeter wave radar;Harmonic analysis;User experience;Millimeter wave communication;Monitoring;Wearable sensors;millimeter wave radar;heart rate recovery monitoring;mmWave sensing;wireless sensing}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICPADS63350.2024.00011}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="10621249" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearBP: Hear Your Blood Pressure via In-ear Acoustic Sensing Based on Heart Sounds</div>
        <!-- Author -->
        <div class="author">
        

        Zhiyuan Zhao,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†Huanran Xie,¬†Kerui Zhang,¬†Li Zhang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2024 - IEEE Conference on Computer Communications</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10621249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Continuous blood pressure (BP) monitoring using wearable devices has received increasing attention due to its importance in diagnosing diseases. However, existing methods mainly measure BP intermittently, involve some form of user effort, and suffer from insufficient accuracy due to sensor properties. In order to overcome these limitations, we study the BP measurement technology based on heart sounds, and find that the time interval between the first and second heart sounds (TIFS) of bone-conducted heart sounds collected in the binaural canal is closely related to BP. Motivated by this, we propose HearBP, a novel BP monitoring system that utilizes inear microphones to collect bone-conducted heart sounds in the binaural canal. We first design a noise removing method based on U-net autoencoder-decoder to separate clean heart sounds from background noises. Then, we design a feature extraction method based on shannon energy and energy-entropy ratio to further mine the time domain and frequency domain features of heart sounds. In addition, combined with the principal component analysis algorithm, we achieve feature dimension reduction to extract the main features related to BP. Finally, we propose a network model based on dendritic neural regression to construct a mapping between the extracted features and BP. Extensive experiments with 41 participants show the average estimation error of 0.97mmHg and 1.61mmHg and the standard deviation error of 3.13mmHg and 3.56mmHg for diastolic pressure and systolic pressure, respectively. These errors are within the acceptable range specified by the FDA‚Äôs AAMI protocol.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10621249</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Zhiyuan and Li, Fan and Xie, Yadong and Xie, Huanran and Zhang, Kerui and Zhang, Li and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2024 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearBP: Hear Your Blood Pressure via In-ear Acoustic Sensing Based on Heart Sounds}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{991-1000}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Heart;Irrigation;Estimation error;Protocols;Frequency-domain analysis;Feature extraction;Time-domain analysis}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM52122.2024.10621249}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TDSC</abbr></div>
<!-- Entry bib key -->
        <div id="10330729" class="col-sm-8">
        <!-- Title -->
        <div class="title">User Authentication on Earable Devices Via Bone-Conducted Occlusion Sounds</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Dependable and Secure Computing (TDSC)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10330729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rapid development of mobile devices and the fast increase of sensitive data, secure and convenient mobile authentication technologies are desired. Except for traditional passwords, many mobile devices have biometric-based authentication methods (e.g., fingerprint, voiceprint, and face recognition), but they are vulnerable to spoofing attacks. To solve this problem, we study new biometric features which are based on the dental occlusion and find that the bone-conducted sound of dental occlusion collected in binaural canals contains unique features of individual bones and teeth. Motivated by this, we propose a novel authentication system, TeethPass^+, which uses earbuds to collect occlusal sounds in binaural canals to achieve authentication. Firstly, we design an event detection method based on spectrum variance to detect bone-conducted sounds. Then, we analyze the time-frequency domain of the sounds to filter out motion noises and extract unique features of users from four aspects: teeth structure, bone structure, occlusal location, and occlusal sound. Finally, we train a Triplet network to construct the user template, which is used to complete authentication. Through extensive experiments including 53 volunteers, the performance of TeethPass^+ in different environments is verified. TeethPass^+ achieves an accuracy of 98.6% and resists 99.7% of spoofing attacks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10330729</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Dependable and Secure Computing (TDSC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{User Authentication on Earable Devices Via Bone-Conducted Occlusion Sounds}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3704-3718}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TDSC.2023.3335368}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="10251599" class="col-sm-8">
        <!-- Title -->
        <div class="title">FingerSlid: Towards Finger-sliding Continuous Authentication on Smart Devices via Vibration</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10251599" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Nowadays, mobile smart devices are widely used in daily life. It is increasingly important to prevent malicious users from accessing private data, thus a secure and convenient authentication method is urgently needed. Compared with common one-off authentication (e.g., password, face recognition, and fingerprint), continuous authentication can provide constant privacy protection. However, most studies are based on behavioral features and vulnerable to spoofing attacks. To solve this problem, we study the unique influence of sliding fingers on active vibration signals, and further propose an authentication system, FingerSlid, which uses vibration motors and accelerometers in mobile devices to sense biometric features of sliding fingers to achieve behavior-independent continuous authentication. First, we design two kinds of active vibration signals and propose a novel signal generation mechanism to improve the anti-attack ability of FingerSlid. Then, we extract different biometric features from the received two kinds of signals, and eliminate the influence of behavioral features in biometric features using a carefully designed Triplet network. Last, user authentication is performed by using the generated behavior-independent biometric features. FingerSlid is evaluated through a large number of experiments under different scenarios, and it achieves an average accuracy of 95.4% and can resist 99.5% of attacks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10251599</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FingerSlid: Towards Finger-sliding Continuous Authentication on Smart Devices via Vibration}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6045-6059}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2023.3315291}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9606582" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearFit+: Personalized Fitness Monitoring via Audio Signals on Smart Speakers</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9606582" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fitness can help to strengthen muscles, increase resistance to diseases, and improve body shape. Nowadays, a great number of people choose to exercise at home/office rather than at the gym due to lack of time. However, it is difficult for them to get good fitness effects without professional guidance. Motivated by this, we propose the first personalized fitness monitoring system, HearFit^++, using smart speakers at home/office. We explore the feasibility of using acoustic sensing to monitor fitness. We design a fitness detection method based on Doppler shift and adopt the short time energy to segment fitness actions. Based on deep learning, HearFit^++ can perform fitness classification and user identification at the same time. Combined with incremental learning, users can easily add new actions. We design 4 evaluation metrics (i.e., duration, intensity, continuity, and smoothness) to help users to improve fitness effects. Through extensive experiments including over 9,000 actions of 10 types of fitness from 12 volunteers, HearFit^++ can achieve an average accuracy of 96.13% on fitness classification and 91% accuracy for user identification. All volunteers confirm that HearFit^++ can help improve the fitness effect in various environments.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9606582</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearFit+: Personalized Fitness Monitoring via Audio Signals on Smart Speakers}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2756--2770}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2021.3125684}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="10229021" class="col-sm-8">
        <!-- Title -->
        <div class="title">WakeUp: Fine-Grained Fatigue Detection Based on Multi-Information Fusion on Smart Speakers</div>
        <!-- Author -->
        <div class="author">
        

        Zhiyuan Zhao,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2023 - IEEE Conference on Computer Communications</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10229021" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the development of society and the gradual increase of life pressure, the number of people engaged in mental work and working hours have increased significantly, resulting in more and more people in a state of fatigue. It not only reduces people‚Äôs work efficiency, but also causes health and safety related problems. The existing fatigue detection systems either have different shortcomings in diverse scenarios or are limited by proprietary equipment, which is difficult to be applied in real life. Motivated by this, we propose a multi-information fatigue detection system named WakeUp based on commercial smart speakers, which is the first to fuse physiological and behavioral information for fine-grained fatigue detection in a non-contact manner. We carefully design a method to simultaneously extract users‚Äô physiological and behavioral information based on the MobileViT network and VMD decomposition algorithm respectively. Then, we design a multi-information fusion method based on the statistical features of these two kinds of information. In addition, we adopt an SVM classifier to achieve fine-grained fatigue level. Extensive experiments with 20 volunteers show that WakeUp can detect fatigue with an accuracy of 97.28%. Meanwhile, WakeUp can maintain stability and robustness under different experimental settings.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10229021</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Zhiyuan and Li, Fan and Xie, Yadong and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2023 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{WakeUp: Fine-Grained Fatigue Detection Based on Multi-Information Fusion on Smart Speakers}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM53939.2023.10229021}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">IOTJ</abbr></div>
<!-- Entry bib key -->
        <div id="9999544" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearASL: Your Smartphone Can Hear American Sign Language</div>
        <!-- Author -->
        <div class="author">
        

        Yusen Wang,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†Chunhui Duan,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Internet of Things Journal (IOTJ)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9999544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Sign language is expressed by movements of the hands and facial expressions, which is mainly used by the deaf community. Although some gesture recognition methods are put forward, they possess different defects and are not applicable to deal with the sign language recognition (SLR) problem. In this article, we propose an end-to-end American SLR system with built-in speakers and microphones in smartphones, which enables SLR at both word level and sentence level. The high-level idea is to use the inaudible acoustic signal to estimate channel information and capture the sign language in real time. We use channel impulse response to represent each sign language gesture, which can realize finger-level recognition. We also pay attention to conversion movements between two words and treat them as an additional label when training the sentence-level classification model. We implement a prototype system and run a series of experiments that demonstrate the promising performance of our system. Experimental results show that our approach can achieve an accuracy of 97.2% at word-level recognition and word error rate of 0.9% at sentence-level recognition, respectively.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9999544</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yusen and Li, Fan and Xie, Yadong and Duan, Chunhui and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Internet of Things Journal (IOTJ)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearASL: Your Smartphone Can Hear American Sign Language}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8839--8852}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/JIOT.2022.3232337}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="10109814" class="col-sm-8">
        <!-- Title -->
        <div class="title">BSMonitor: Noise-Resistant Bowel Sound Monitoring Via Earphones</div>
        <!-- Author -->
        <div class="author">
        

        Zhiyuan Zhao,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10109814" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Bowel sound (BS) is an important physiological signal of the human body, which is also an objective reflection of gastrointestinal motility. However, BS has characteristics of weak signal, strong noise, and randomicity, which bring great challenges to the daily detection of BS. In this paper, we propose BSMonitor, the first BS monitoring system with strong noise-resistant capability via earphones. BSMonitor uses one earphone attached to the abdomen to collect BS signals and the other earphone worn in the ear to collect external noises and internal noises. After eliminating the noises through the Kalman filter and band-pass filter, the signal containing BS is separated via the empirical mode decomposition. Then BSMonitor extracts MFCC features of BS signals and applies a carefully-designed LSTM network to perform highly-accurate BS detection. Finally, an alert mechanism calculates the frequency and duration of detected BS and compares with the normal values to alert users. Furthermore, to increase the amount and diversity of training data, we introduce a data augmentation method, which can further improve the accuracy and generalization of BSMonitor. Through extensive experiments with 18 volunteers, we find that BSMonitor not only achieves high accuracy of BS detection but also has strong generalization across different users and environments. Particularly, BSMonitor achieves accuracy up to 98.73% and 94.56% in the benchmark experiments and the cross experiments, respectively.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10109814</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Zhiyuan and Li, Fan and Xie, Yadong and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BSMonitor: Noise-Resistant Bowel Sound Monitoring Via Earphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--15}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2023.3270926}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="10228976" class="col-sm-8">
        <!-- Title -->
        <div class="title">FlyTracker: Motion Tracking and Obstacle Detection for Drones Using Event Cameras</div>
        <!-- Author -->
        <div class="author">
        

        Yue Wu,¬†Jingao Xu,¬†Danyang Li,¬†<b><em>Yadong Xie</em></b>,¬†Hao Cao,¬†Fan Li,¬†and¬†Zheng Yang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2023 - IEEE Conference on Computer Communications</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10228976" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Location awareness in environments is one of the key parts for drones‚Äô applications and have been explored through various visual sensors. However, standard cameras easily suffer from motion blur under high moving speeds and low-quality image under poor illumination, which brings challenges for drones to perform motion tracking. Recently, a kind of bio-inspired sensors called event cameras emerge, offering advantages like high temporal resolution, high dynamic range and low latency, which motivate us to explore their potential to perform motion tracking in limited scenarios. In this paper, we propose FlyTracker, aiming at developing visual sensing ability for drones of both individual and circumambient location-relevant contextual, by using a monocular event camera. In FlyTracker, background-subtraction-based method is proposed to distinguish moving objects from background and fusion-based photometric features are carefully designed to obtain motion information. Through multilevel fusion of events and images, which are heterogeneous visual data, FlyTracker can effectively and reliably track the 6-DoF pose of the drone as well as monitor relative positions of moving obstacles. We evaluate performance of FlyTracker in different environments and the results show that FlyTracker is more accurate than the state-of-the-art baselines.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10228976</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yue and Xu, Jingao and Li, Danyang and Xie, Yadong and Cao, Hao and Li, Fan and Yang, Zheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2023 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FlyTracker: Motion Tracking and Obstacle Detection for Drones Using Event Cameras}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM53939.2023.10228976}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TOSN</abbr></div>
<!-- Entry bib key -->
        <div id="10.1145/3517014" class="col-sm-8">
        <!-- Title -->
        <div class="title">SymListener: Detecting Respiratory Symptoms via Acoustic Sensing in Driving Environments</div>
        <!-- Author -->
        <div class="author">
        

        Yue Wu,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†Yu Wang,¬†and¬†Zheng Yang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>ACM Transactions on Sensor Networks (TOSN)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://doi.org/10.1145/3517014" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Sound-related respiratory symptoms are commonly observed in our daily lives. They are closely related to illnesses, infections, or allergies but ignored by the majority. Existing detection methods either depend on specific devices, which are inconvenient to wear, or are sensitive to noises and only work for indoor environment. Considering the lack of monitoring method for in-car environment, where there is high risk of spreading infectious diseases, we propose a smartphone-based system, named SymListener, to detect respiratory symptoms in driving environment. By continuously recording acoustic data through a built-in microphone, SymListener can detect the sounds of cough, sneeze, and sniffle. We design a modified ABSE-based method to remove the strong and changeable driving noises while saving energy of the smartphone. An LSTM network is adopted to classify the three types of symptoms according to the carefully designed acoustic features. We implement SymListener on different Android devices and evaluate its performance in real driving environment. The evaluation results show that SymListener can reliably detect target respiratory symptoms with an average accuracy of 92.19% and an average precision of 90.91%.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3517014</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yue and Li, Fan and Xie, Yadong and Wang, Yu and Yang, Zheng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SymListener: Detecting Respiratory Symptoms via Acoustic Sensing in Driving Environments}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3517014}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Sensor Networks (TOSN)}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{1--21}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9312460" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearSmoking: Smoking Detection in Driving Environment via Acoustic Sensing on Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9312460" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Driving safety has drawn much public attention in recent years due to the fast-growing number of cars. Smoking is one of the threats to driving safety but is often ignored by drivers. Existing works on smoking detection either work in contact manner or need additional devices. This motivates us to explore the practicability of using smartphones to detect smoking events in driving environment. In this paper, we propose a cigarette smoking detection system, named HearSmoking, which only uses acoustic sensors on smartphones to improve driving safety. After investigating typical smoking habits of drivers, including hand movement and chest fluctuation, we design an acoustic signal to be emitted by the speaker and received by the microphone. We calculate Relative Correlation Coefficient of received signals to obtain movement patterns of hands and chest. The processed data is sent into a trained Convolutional Neural Network for classification of hand movement. We also design a method to detect respiration at the same time. To improve system performance, we further analyse the periodicity of the composite smoking motion. Through extensive experiments in real driving environments, HearSmoking detects smoking events with an average total accuracy of 93.44 percent in real-time.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9312460</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearSmoking: Smoking Detection in Driving Environment via Acoustic Sensing on Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2847--2860}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2020.3048785}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="9796951" class="col-sm-8">
        <!-- Title -->
        <div class="title">TeethPass: Dental Occlusion-based User Authentication via In-ear Acoustic Sensing</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Huijie Chen,¬†Zhiyuan Zhao,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2022 - IEEE Conference on Computer Communications</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9796951" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rapid development of mobile devices and the fast increase of sensitive data, secure and convenient mobile authentication technologies are desired. Except for traditional passwords, many mobile devices have biometric-based authentication methods (e.g., fingerprint, voiceprint, and face recognition), but they are vulnerable to spoofing attacks. To solve this problem, we study new biometric features which are based on the dental occlusion and find that the bone-conducted sound of dental occlusion collected in binaural canals contains unique features of individual bones and teeth. Motivated by this, we propose a novel authentication system, TeethPass, which uses earbuds to collect occlusal sounds in binaural canals to achieve authentication. We design an event detection method based on spectrum variance and double thresholds to detect bone-conducted sounds. Then, we analyze the time-frequency domain of the sounds to filter out motion noises and extract unique features of users from three aspects: bone structure, occlusal location, and occlusal sound. Finally, we design an incremental learning-based Siamese network to construct the classifier. Through extensive experiments including 22 participants, the performance of TeethPass in different environments is verified. TeethPass achieves an accuracy of 96.8% and resists nearly 99% of spoofing attacks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">9796951</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Chen, Huijie and Zhao, Zhiyuan and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2022 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TeethPass: Dental Occlusion-based User Authentication via In-ear Acoustic Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1789--1798}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM48880.2022.9796951}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">IOTJ</abbr></div>
<!-- Entry bib key -->
        <div id="9488277" class="col-sm-8">
        <!-- Title -->
        <div class="title">Gait and Respiration-Based User Identification Using Wi-Fi Signal</div>
        <!-- Author -->
        <div class="author">
        

        Xiaoyang Wang,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Internet of Things Journal (IOTJ)</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9488277" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The ever-growing security issues in various scenarios create an urgent demand for a reliable and convenient identification system. Traditional identification systems request users to provide passwords, fingerprints, or other easily stolen information. Existing works show that everyone‚Äôs gait and respiration have unique characteristics and are difficult to imitate. But these works only use gait or respiration information to achieve identification, which leads to low accuracy or long identification time. And they have no strong anti-interference ability, which leads to the limitation in practical application. Toward this end, we propose a new system which uses both gait and respiratory biometric characteristics to achieve user identification using Wi-Fi (GRi-Fi) in the presence of interferences. In our system, we design a segmentation algorithm to segment gait and respiration data. And we design a weighted subcarrier screening method to improve the anti-interference ability. In order to shorten the identification time, we propose a feature integration method based on the weighted average. Finally, we use a deep learning method to identify users accurately. Experimental results show that GRi-Fi can identify the users identity with an average accuracy of 98.3% in noninterference environments. Even in the presence of multiple interferences, the average identification accuracy also reaches 91.2%. In future applications, our system can be applied to many fields of Internet of Things, such as smart home systems and clocking in at companies.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9488277</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Xiaoyang and Li, Fan and Xie, Yadong and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Internet of Things Journal (IOTJ)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gait and Respiration-Based User Identification Using Wi-Fi Signal}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3509--3521}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/JIOT.2021.3097892}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9311795" class="col-sm-8">
        <!-- Title -->
        <div class="title">HDSpeed: Hybrid Detection of Vehicle Speed via Acoustic Sensing on Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        Yue Wu,¬†Fan Li,¬†<b><em>Yadong Xie</em></b>,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9311795" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Speeding is one of the biggest threatens to road safety. However, facilities like radar detector and speed camera are not deployed everywhere, as roads in some areas like campus and residential areas often lack these facilities. Several solutions either depend on pre-deployed infrastructures, or require additional devices, which motivate us to explore the practicability of using smartphones‚Äô acoustic sensors to detect vehicle speed. In this paper, we propose a Hybrid Detection system for vehicle Speed (HDSpeed). We first investigate the relationship between acoustic pattern and vehicle speed. According to our findings on typical patterns of both electric vehicles (EVs) and gasoline vehicles (GVs), we separately extract different features from the acoustic signals of EVs and GVs. A CNN and an LSTMN are designed for training EV and GV models, respectively. Considering that applying neural networks obtains coarse-grained information like a speed section, we propose a detection method based on active acoustic sensing, in which method HDSpeed calculates the fine-grained speed by detecting the distance change between the smartphone and the passing vehicle. In addition, the previously detected speed section can eliminate interferences of surrounding moving objects. Through extensive experiments in real driving environments, HDSpeed achieves an average error of 2.17km/h.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9311795</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yue and Li, Fan and Xie, Yadong and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HDSpeed: Hybrid Detection of Vehicle Speed via Acoustic Sensing on Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2833--2846}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2020.3048380}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="9488811" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearFit: Fitness Monitoring on Smart Speakers via Active Acoustic Sensing</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2021 - IEEE Conference on Computer Communications</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9488811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fitness can help to strengthen muscles, increase resistance to diseases and improve body shape. Nowadays, more and more people tend to exercise at home/office, since they lack time to go to the dedicated gym. However, it is difficult for most of them to get good fitness effect due to the lack of professional guidance. Motivated by this, we propose HearFit, the first non-invasive fitness monitoring system based on commercial smart speakers for home/office environments. To achieve this, we turn smart speakers into active sonars. We design a fitness detection method based on Doppler shift and adopt the short time energy to segment fitness actions. We design a high-accuracy LSTM network to determine the type of fitness. Combined with incremental learning, users can easily add new actions. Finally, we evaluate the local (i.e., intensity and duration) and global (i.e., continuity and smoothness) fitness quality of users to help to improve fitness effect and prevent injury. Through extensive experiments including over 7,000 actions of 10 types of fitness with and without dumbbells from 12 participants, HearFit can detect fitness actions with an average accuracy of 96.13%, and give accurate statistics in various environments.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">9488811</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearFit: Fitness Monitoring on Smart Speakers via Active Acoustic Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM42981.2021.9488811}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9055089" class="col-sm-8">
        <!-- Title -->
        <div class="title">Real-Time Detection for Drowsy Driving via Acoustic Sensing on Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9055089" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Drowsy driving is one of the biggest threats to driving safety, which has drawn much public attention in recent years. Thus, a simple but robust system that can remind drivers of drowsiness levels with off-the-shelf devices (e.g., smartphones) is very necessary. With this motivation, we explore the feasibility of using acoustic sensors on smartphones to detect drowsy driving. Through analyzing real driving data to study characteristics of drowsy driving, we find some unique patterns of Doppler shift caused by three typical drowsy behaviours (i.e., nodding, yawning and operating steering wheel), among which operating steering wheels is also related to drowsiness levels. Then, a real-time Drowsy Driving Detection system named D 3 -Guard is proposed based on the acoustic sensing abilities of smartphones. We adopt several effective feature extraction methods, and carefully design a high-accuracy detector based on LSTM networks for the early detection of drowsy driving. Besides, measures to distinguish drowsiness levels are also introduced in the system by analyzing the data of operating steering wheel. Through extensive experiments with five drivers in real driving environments, D 3 -Guard detects drowsy driving actions with an average accuracy of 93.31%, as well as classifies drowsiness levels with an average accuracy of 86%.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9055089</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-Time Detection for Drowsy Driving via Acoustic Sensing on Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2671--2685}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2020.2984278}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">Inf. Sci.</abbr></div>
<!-- Entry bib key -->
        <div id="WEI2021109" class="col-sm-8">
        <!-- Title -->
        <div class="title">SVSV: Online handwritten signature verification based on sound and vibration</div>
        <!-- Author -->
        <div class="author">
        

        Zhixiang Wei,¬†Song Yang,¬†<b><em>Yadong Xie</em></b>,¬†Fan Li,¬†and¬†Bo Zhao</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Information Sciences</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0020025521004400" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Handwritten signature is one of the most important behavioral biometrics and plays an important role in the field of identity verification. It is regarded as a legal means to verify personal identity by administrative and financial institutions. Traditional manual signature verification requires large labor costs and the probability of verification error is relatively high. Nowadays, tablets are often used for signature capturing, which motivates us to explore the feasibility of using tablets for signature verification. In this paper, we propose an online handwriting signature verification system based on sound and vibration (SVSV) generated during the signing process. We develop an application to collect signature-related vibration and sound data. We first extract the time domain features of the sound signal and use Fast Fourier Transform to extract the frequency domain features of the sound data. For vibration data, we use Discrete Cosine Transform for dimensionality reduction and feature extraction. Then we fuse the sound and vibration features. Finally, we design an efficient one-class classifier based on the Convolutional Neural Network to perform signature verification. Through extensive experiments with 12 volunteers, the results show that SVSV is a robust and efficient system with an AUC of 0.984 and an EER of 0.05.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">WEI2021109</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SVSV: Online handwritten signature verification based on sound and vibration}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Information Sciences}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{572}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{109--125}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0020--0255}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.ins.2021.04.099}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wei, Zhixiang and Yang, Song and Xie, Yadong and Li, Fan and Zhao, Bo}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="8737470" class="col-sm-8">
        <!-- Title -->
        <div class="title">D3-Guard: Acoustic-based Drowsy Driving Detection Using Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2019 - IEEE Conference on Computer Communications</em>, 2019
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/8737470" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Since the number of cars has grown rapidly in recent years, driving safety draws more and more public attention. Drowsy driving is one of the biggest threatens to driving safety. Therefore, a simple but robust system that can detect drowsy driving with commercial off-the-shelf devices (such as smart-phones) is very necessary. With this motivation, we explore the feasibility of purely using acoustic sensors embedded in smart-phones to detect drowsy driving. We first study characteristics of drowsy driving, and find some unique patterns of Doppler shift caused by three typical drowsy behaviors, i.e., nodding, yawning and operating steering wheel. We then validate our important findings through empirical analysis of the driving data collected from real driving environments. We further propose a real-time Drowsy Driving Detection system (D 3 -Guard) based on audio devices embedded in smartphones. In order to improve the performance of our system, we adopt an effective feature extraction method based on undersampling technique and FFT, and carefully design a high-accuracy detector based on LSTM networks for the early detection of drowsy driving. Through extensive experiments with 5 volunteer drivers in real driving environments, our system can distinguish drowsy driving actions with an average total accuracy of 93.31% in real-time. Over 80% drowsy driving actions can be detected within first 70% of action duration.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8737470</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2019 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{D3-Guard: Acoustic-based Drowsy Driving Detection Using Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1225--1233}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM.2019.8737470}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">IOTJ</abbr></div>
<!-- Entry bib key -->
        <div id="wu2019context" class="col-sm-8">
        <!-- Title -->
        <div class="title">A context-aware multiarmed bandit incentive mechanism for mobile crowd sensing systems</div>
        <!-- Author -->
        <div class="author">
        

        Yue Wu,¬†Fan Li,¬†Liran Ma,¬†<b><em>Yadong Xie</em></b>,¬†Ting Li,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Internet of Things Journal (IOTJ)</em>, 2019
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8660468" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Smart city is a key component in Internet of Things, so it has attracted much attention. The emergence of mobile crowd sensing (MCS) systems enables many smart city applications. In an MCS system, sensing tasks are allocated to a number of mobile users. As a result, the sensing related context of each mobile user plays a significant role on service quality. However, some important sensing context is ignored in the literature. This motivates us to propose a context-aware multiarmed bandit (C-MAB) incentive mechanism to facilitate quality-based worker selection in an MCS system. We evaluate a worker‚Äôs service quality by its context (i.e., extrinsic ability and intrinsic ability) and cost. Based on our proposed C-MAB incentive mechanism and quality evaluation design, we develop a modified Thompson sampling worker selection (MTS-WS) algorithm to select workers in a reinforcement learning manner. MTS-WS is able to choose effective workers because it can maintain accurate worker quality information by updating evaluation parameters according to the status of task accomplishment. We theoretically prove that our C-MAB incentive mechanism is selection efficient, computationally efficient, individually rational, and truthful. Finally, we evaluate our MTS-WS algorithm on simulated and real-world datasets in comparison with some other classic algorithms. Our evaluation results demonstrate that MTS-WS achieves the highest cumulative utility of the requester and social welfare.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wu2019context</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A context-aware multiarmed bandit incentive mechanism for mobile crowd sensing systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yue and Li, Fan and Ma, Liran and Xie, Yadong and Li, Ting and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Internet of Things Journal (IOTJ)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7648--7658}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/JIOT.2019.2903197}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

</div>

  </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Yadong  Xie. Last updated: August, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="/assets/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="/assets/js/mdb.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    

<!-- WeChat Modal -->

<script>
var wechatModal = document.getElementById("WeChatMod");
var wechatBtn = document.getElementById("WeChatBtn");

wechatBtn.onclick = function() {
  wechatModal.style.display = "block";
}

window.onclick = function(event) {
  if (event.target == wechatModal) {
    wechatModal.style.display = "none";
  }
}
</script>


  </body>
</html>
