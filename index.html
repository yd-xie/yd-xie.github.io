<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yadong Xie</title>
    <meta name="author" content="Yadong  Xie">
    <meta name="description" content="">
    <!-- OpenGraph -->
    <meta property="og:site_name" content="Yadong Xie">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Yadong Xie | About">
    <meta property="og:url" content="https://yd-xie.github.io/">
    <meta property="og:description" content="">
    
    <meta property="og:locale" content="en">

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="About">
    <meta name="twitter:description" content="">
    
    

    <!-- Schema.org -->
    <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Yadong  Xie"
        },
        "url": "https://yd-xie.github.io/",
        "@type": "WebSite",
        "description": "",
        "headline": "About",
        "sameAs": ["https://orcid.org/0000-0002-2467-4240", "https://scholar.google.com/citations?user=bi8AlbMAAAAJ", "https://www.researchgate.net/profile/Yadong-Xie-2"],
        "name": "Yadong  Xie",
        "@context": "https://schema.org"
    }
    </script>


    <!-- Bootstrap & MDB -->
    <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
    <!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light">

    

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://yd-xie.github.io/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/news/">News</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">Awards</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fa-solid fa-moon"></i>
                  <i class="fa-solid fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
            Yadong Xie
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic.jpg?4b7223ed81cb538fa8418943f38531de" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>

            <div class="more-info" style="text-align:center; font-weight:bold">
              <p>Postdoctoral Researcher</p> <p>School of Software</p> <p>Tsinghua University</p> <p>Email: yad.xie@gmail.com</p>

            </div>
          </div>

          <div class="clearfix">
            <p><big> I am currently a postdoctoral researcher in the <a href="http://tns.thss.tsinghua.edu.cn/sun/index.html" rel="external nofollow noopener" target="_blank">Systems and Ubiquitous Networking (SUN) group</a> in Tsinghua University, advised by <a href="http://tns.thss.tsinghua.edu.cn/sun/members/YuanHe/" rel="external nofollow noopener" target="_blank">Dr. Yuan He</a>. I received my Ph.D. degree in the School of Computer Science, Beijing Institute of Technology in 2022, under supervision of <a href="https://fli-bit.github.io/#Home" rel="external nofollow noopener" target="_blank">Prof. Fan Li</a>. </big></p>

<font size="+3"> Research Interests </font>
<p><big> My research interests include Acoustic Sensing, Wireless Sensing, Privacy-Preserving Authentication, and Emerging Sensing Technologies. </big></p>
<ul>
<li> <big> <strong> Acoustic Sensing: </strong> WDevice-free activity recognition, physiological signal monitoring, and acoustic backscatter systems. </big> </li>
<li> <big> <strong> Wireless Sensing: </strong> Wi-Fi and mmWave based sensing for human activity and vital sign monitoring. </big> </li>
<li> <big> <strong> Privacy-Preserving Authentication:</strong> User authentication and continuous identity verification leveraging multimodal sensing. </big> </li>
<li> <big> <strong> Emerging Sensing Technologies:</strong> Metasurface-assisted sensing, event-camera-based tracking, and novel cross-modal approaches. </big> </li>
</ul>

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">Latest News</a></h2>          <div class="news">
            <div class="table-responsive" style="max-height: 60vw">
              <table class="table table-sm table-borderless table-hover">
              
                <tr>
                  <th scope="row" style="width: 20%">Jul 20, 2025</th>
                  <td>
                    Our work is accepted by ACM SIGCOMM 2025!

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Jun 21, 2025</th>
                  <td>
                    Our work MASS is accepted by WASA 2025!

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Oct 10, 2024</th>
                  <td>
                    Our work mmHRR and mmjaw is accepted by IEEE ICPADS 2024!

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Jul 11, 2024</th>
                  <td>
                    Yadong Xie has been honored with the ÂõΩÂÆ∂ËµÑÂä©ÂçöÂ£´ÂêéÁ†îÁ©∂‰∫∫ÂëòËÆ°ÂàíBÊ°£ËµÑÂä©

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Jan 5, 2024</th>
                  <td>
                    Yadong Xie has been honored with the Âåó‰∫¨Â∏ÇÁßëÂçèÈùíÂπ¥‰∫∫ÊâçÊâò‰∏æÂ∑•Á®ã

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Dec 1, 2023</th>
                  <td>
                    Our work HearBP is accepted by IEEE INFOCOM 2024!

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">SIGCOMM</abbr></div>
<!-- Entry bib key -->
        <div id="Sigcomm_acoustic" class="col-sm-8">
        <!-- Title -->
        <div class="title">Acoustic Backscatter Network for Vehicle Body-in-White</div>
        <!-- Author -->
        <div class="author">
        

        Weiguo Wang,¬†Yuan He,¬†<b><em>Yadong Xie</em></b>,¬†Chuyue Xie,¬†Yi Kai,¬†and¬†Chengchen Hu</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2025 ACM Special Interest Group on Data Communication (SIGCOMM)</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a novel approach to monitor the Body in White (BiW), the fundamental metallic structure of a vehicle. Existing monitoring methods, including both wired and wireless sensor systems, face significant challenges due to integration complexity, weight considerations, material costs, and signal blockage within the metallic environment. To overcome these limitations, we introduce Arach-Net, an acoustic backscatter network that leverages the conductive properties of the BiW to propagate vibration signals for energy transfer and data communication. This system comprises batteryfree tags that harvest energy from BiW vibrations and utilize a backscatter technique for efficient communication, thereby eliminating the need for external power sources and reducing the power consumption. We address key challenges such as power sufficiency for tag activation and sustained operation, and collision reduction in network communication, by designing an ultra-low power backscatter tag and a distributed slot allocation protocol. We implement ArachNet, and deploy 12 tags onto the BiW of an electric SUV car. The evaluation results show that the power consumption of the tag is 51.0 ùúáW for uplink packet transmission, and 24.8 ùúáW for downlink packet reception. With our network protocol, the slot utilization can be up to 81.2%.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sigcomm_acoustic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Acoustic Backscatter Network for Vehicle Body-in-White}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Weiguo and He, Yuan and Xie, Yadong and Xie, Chuyue and Kai, Yi and Hu, Chengchen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 ACM Special Interest Group on Data Communication (SIGCOMM)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TDSC</abbr></div>
<!-- Entry bib key -->
        <div id="10330729" class="col-sm-8">
        <!-- Title -->
        <div class="title">User Authentication on Earable Devices Via Bone-Conducted Occlusion Sounds</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Dependable and Secure Computing (TDSC)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10330729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rapid development of mobile devices and the fast increase of sensitive data, secure and convenient mobile authentication technologies are desired. Except for traditional passwords, many mobile devices have biometric-based authentication methods (e.g., fingerprint, voiceprint, and face recognition), but they are vulnerable to spoofing attacks. To solve this problem, we study new biometric features which are based on the dental occlusion and find that the bone-conducted sound of dental occlusion collected in binaural canals contains unique features of individual bones and teeth. Motivated by this, we propose a novel authentication system, TeethPass^+, which uses earbuds to collect occlusal sounds in binaural canals to achieve authentication. Firstly, we design an event detection method based on spectrum variance to detect bone-conducted sounds. Then, we analyze the time-frequency domain of the sounds to filter out motion noises and extract unique features of users from four aspects: teeth structure, bone structure, occlusal location, and occlusal sound. Finally, we train a Triplet network to construct the user template, which is used to complete authentication. Through extensive experiments including 53 volunteers, the performance of TeethPass^+ in different environments is verified. TeethPass^+ achieves an accuracy of 98.6% and resists 99.7% of spoofing attacks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10330729</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Dependable and Secure Computing (TDSC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{User Authentication on Earable Devices Via Bone-Conducted Occlusion Sounds}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3704-3718}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TDSC.2023.3335368}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="10251599" class="col-sm-8">
        <!-- Title -->
        <div class="title">FingerSlid: Towards Finger-sliding Continuous Authentication on Smart Devices via Vibration</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10251599" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Nowadays, mobile smart devices are widely used in daily life. It is increasingly important to prevent malicious users from accessing private data, thus a secure and convenient authentication method is urgently needed. Compared with common one-off authentication (e.g., password, face recognition, and fingerprint), continuous authentication can provide constant privacy protection. However, most studies are based on behavioral features and vulnerable to spoofing attacks. To solve this problem, we study the unique influence of sliding fingers on active vibration signals, and further propose an authentication system, FingerSlid, which uses vibration motors and accelerometers in mobile devices to sense biometric features of sliding fingers to achieve behavior-independent continuous authentication. First, we design two kinds of active vibration signals and propose a novel signal generation mechanism to improve the anti-attack ability of FingerSlid. Then, we extract different biometric features from the received two kinds of signals, and eliminate the influence of behavioral features in biometric features using a carefully designed Triplet network. Last, user authentication is performed by using the generated behavior-independent biometric features. FingerSlid is evaluated through a large number of experiments under different scenarios, and it achieves an average accuracy of 95.4% and can resist 99.5% of attacks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10251599</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FingerSlid: Towards Finger-sliding Continuous Authentication on Smart Devices via Vibration}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6045-6059}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2023.3315291}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9606582" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearFit+: Personalized Fitness Monitoring via Audio Signals on Smart Speakers</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9606582" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fitness can help to strengthen muscles, increase resistance to diseases, and improve body shape. Nowadays, a great number of people choose to exercise at home/office rather than at the gym due to lack of time. However, it is difficult for them to get good fitness effects without professional guidance. Motivated by this, we propose the first personalized fitness monitoring system, HearFit^++, using smart speakers at home/office. We explore the feasibility of using acoustic sensing to monitor fitness. We design a fitness detection method based on Doppler shift and adopt the short time energy to segment fitness actions. Based on deep learning, HearFit^++ can perform fitness classification and user identification at the same time. Combined with incremental learning, users can easily add new actions. We design 4 evaluation metrics (i.e., duration, intensity, continuity, and smoothness) to help users to improve fitness effects. Through extensive experiments including over 9,000 actions of 10 types of fitness from 12 volunteers, HearFit^++ can achieve an average accuracy of 96.13% on fitness classification and 91% accuracy for user identification. All volunteers confirm that HearFit^++ can help improve the fitness effect in various environments.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9606582</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearFit+: Personalized Fitness Monitoring via Audio Signals on Smart Speakers}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2756--2770}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2021.3125684}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9312460" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearSmoking: Smoking Detection in Driving Environment via Acoustic Sensing on Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9312460" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Driving safety has drawn much public attention in recent years due to the fast-growing number of cars. Smoking is one of the threats to driving safety but is often ignored by drivers. Existing works on smoking detection either work in contact manner or need additional devices. This motivates us to explore the practicability of using smartphones to detect smoking events in driving environment. In this paper, we propose a cigarette smoking detection system, named HearSmoking, which only uses acoustic sensors on smartphones to improve driving safety. After investigating typical smoking habits of drivers, including hand movement and chest fluctuation, we design an acoustic signal to be emitted by the speaker and received by the microphone. We calculate Relative Correlation Coefficient of received signals to obtain movement patterns of hands and chest. The processed data is sent into a trained Convolutional Neural Network for classification of hand movement. We also design a method to detect respiration at the same time. To improve system performance, we further analyse the periodicity of the composite smoking motion. Through extensive experiments in real driving environments, HearSmoking detects smoking events with an average total accuracy of 93.44 percent in real-time.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9312460</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearSmoking: Smoking Detection in Driving Environment via Acoustic Sensing on Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2847--2860}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2020.3048785}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="9796951" class="col-sm-8">
        <!-- Title -->
        <div class="title">TeethPass: Dental Occlusion-based User Authentication via In-ear Acoustic Sensing</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Huijie Chen,¬†Zhiyuan Zhao,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2022 - IEEE Conference on Computer Communications</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9796951" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rapid development of mobile devices and the fast increase of sensitive data, secure and convenient mobile authentication technologies are desired. Except for traditional passwords, many mobile devices have biometric-based authentication methods (e.g., fingerprint, voiceprint, and face recognition), but they are vulnerable to spoofing attacks. To solve this problem, we study new biometric features which are based on the dental occlusion and find that the bone-conducted sound of dental occlusion collected in binaural canals contains unique features of individual bones and teeth. Motivated by this, we propose a novel authentication system, TeethPass, which uses earbuds to collect occlusal sounds in binaural canals to achieve authentication. We design an event detection method based on spectrum variance and double thresholds to detect bone-conducted sounds. Then, we analyze the time-frequency domain of the sounds to filter out motion noises and extract unique features of users from three aspects: bone structure, occlusal location, and occlusal sound. Finally, we design an incremental learning-based Siamese network to construct the classifier. Through extensive experiments including 22 participants, the performance of TeethPass in different environments is verified. TeethPass achieves an accuracy of 96.8% and resists nearly 99% of spoofing attacks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">9796951</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Chen, Huijie and Zhao, Zhiyuan and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2022 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TeethPass: Dental Occlusion-based User Authentication via In-ear Acoustic Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1789--1798}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM48880.2022.9796951}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="9488811" class="col-sm-8">
        <!-- Title -->
        <div class="title">HearFit: Fitness Monitoring on Smart Speakers via Active Acoustic Sensing</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2021 - IEEE Conference on Computer Communications</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9488811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fitness can help to strengthen muscles, increase resistance to diseases and improve body shape. Nowadays, more and more people tend to exercise at home/office, since they lack time to go to the dedicated gym. However, it is difficult for most of them to get good fitness effect due to the lack of professional guidance. Motivated by this, we propose HearFit, the first non-invasive fitness monitoring system based on commercial smart speakers for home/office environments. To achieve this, we turn smart speakers into active sonars. We design a fitness detection method based on Doppler shift and adopt the short time energy to segment fitness actions. We design a high-accuracy LSTM network to determine the type of fitness. Combined with incremental learning, users can easily add new actions. Finally, we evaluate the local (i.e., intensity and duration) and global (i.e., continuity and smoothness) fitness quality of users to help to improve fitness effect and prevent injury. Through extensive experiments including over 7,000 actions of 10 types of fitness with and without dumbbells from 12 participants, HearFit can detect fitness actions with an average accuracy of 96.13%, and give accurate statistics in various environments.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">9488811</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HearFit: Fitness Monitoring on Smart Speakers via Active Acoustic Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM42981.2021.9488811}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">TMC</abbr></div>
<!-- Entry bib key -->
        <div id="9055089" class="col-sm-8">
        <!-- Title -->
        <div class="title">Real-Time Detection for Drowsy Driving via Acoustic Sensing on Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9055089" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Drowsy driving is one of the biggest threats to driving safety, which has drawn much public attention in recent years. Thus, a simple but robust system that can remind drivers of drowsiness levels with off-the-shelf devices (e.g., smartphones) is very necessary. With this motivation, we explore the feasibility of using acoustic sensors on smartphones to detect drowsy driving. Through analyzing real driving data to study characteristics of drowsy driving, we find some unique patterns of Doppler shift caused by three typical drowsy behaviours (i.e., nodding, yawning and operating steering wheel), among which operating steering wheels is also related to drowsiness levels. Then, a real-time Drowsy Driving Detection system named D 3 -Guard is proposed based on the acoustic sensing abilities of smartphones. We adopt several effective feature extraction methods, and carefully design a high-accuracy detector based on LSTM networks for the early detection of drowsy driving. Besides, measures to distinguish drowsiness levels are also introduced in the system by analyzing the data of operating steering wheel. Through extensive experiments with five drivers in real driving environments, D 3 -Guard detects drowsy driving actions with an average accuracy of 93.31%, as well as classifies drowsiness levels with an average accuracy of 86%.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9055089</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Mobile Computing (TMC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-Time Detection for Drowsy Driving via Acoustic Sensing on Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2671--2685}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TMC.2020.2984278}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 abbr"><abbr class="badge">INFOCOM</abbr></div>
<!-- Entry bib key -->
        <div id="8737470" class="col-sm-8">
        <!-- Title -->
        <div class="title">D3-Guard: Acoustic-based Drowsy Driving Detection Using Smartphones</div>
        <!-- Author -->
        <div class="author">
        

        <b><em>Yadong Xie</em></b>,¬†Fan Li,¬†Yue Wu,¬†Song Yang,¬†and¬†Yu Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE INFOCOM 2019 - IEEE Conference on Computer Communications</em>, 2019
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/8737470" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Since the number of cars has grown rapidly in recent years, driving safety draws more and more public attention. Drowsy driving is one of the biggest threatens to driving safety. Therefore, a simple but robust system that can detect drowsy driving with commercial off-the-shelf devices (such as smart-phones) is very necessary. With this motivation, we explore the feasibility of purely using acoustic sensors embedded in smart-phones to detect drowsy driving. We first study characteristics of drowsy driving, and find some unique patterns of Doppler shift caused by three typical drowsy behaviors, i.e., nodding, yawning and operating steering wheel. We then validate our important findings through empirical analysis of the driving data collected from real driving environments. We further propose a real-time Drowsy Driving Detection system (D 3 -Guard) based on audio devices embedded in smartphones. In order to improve the performance of our system, we adopt an effective feature extraction method based on undersampling technique and FFT, and carefully design a high-accuracy detector based on LSTM networks for the early detection of drowsy driving. Through extensive experiments with 5 volunteer drivers in real driving environments, our system can distinguish drowsy driving actions with an average total accuracy of 93.31% in real-time. Over 80% drowsy driving actions can be detected within first 70% of action duration.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8737470</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yadong and Li, Fan and Wu, Yue and Yang, Song and Wang, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE INFOCOM 2019 - IEEE Conference on Computer Communications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{D3-Guard: Acoustic-based Drowsy Driving Detection Using Smartphones}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1225--1233}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/INFOCOM.2019.8737470}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- awards -->
          <h2><a href="/awards/" style="color: inherit;">Latest Awards</a></h2>          <div class="awards">
            <div class="table-responsive" style="max-height: 60vw">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row" style="width: 20%">Jul 11, 2024</th>
                  <td>
                    ‰∏≠ÂõΩÂçöÂ£´ÂêéÁßëÂ≠¶Âü∫Èáë‰ºö, ÂõΩÂÆ∂ËµÑÂä©ÂçöÂ£´ÂêéÁ†îÁ©∂‰∫∫ÂëòËÆ°ÂàíBÊ°£ËµÑÂä©, 2024-2025Âπ¥

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Jan 5, 2024</th>
                  <td>
                    Âåó‰∫¨Â∏ÇÁßëÂ≠¶ÊäÄÊúØÂçè‰ºö, Âåó‰∫¨Â∏ÇÁßëÂçèÈùíÂπ¥‰∫∫ÊâçÊâò‰∏æÂ∑•Á®ã, 2024-2025Âπ¥

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Mar 5, 2022</th>
                  <td>
                    IEEE INFOCOM-2022 Student Grant Award

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Nov 21, 2021</th>
                  <td>
                    The Special Scholarship, Beijing Institute of Technology <br>
Âåó‰∫¨ÁêÜÂ∑•Â§ßÂ≠¶, ÁâπÁ≠âÂ≠¶‰∏öÂ•ñÂ≠¶Èáë

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Nov 15, 2021</th>
                  <td>
                    National Scholarship, Ministry of Education <br>
ÊïôËÇ≤ÈÉ®, ÂõΩÂÆ∂Â•ñÂ≠¶ÈáëÔºàÊéíÂêçÁ¨¨‰∏ÄÔºâ

                  </td>
                </tr>
                <tr>
                  <th scope="row" style="width: 20%">Mar 5, 2021</th>
                  <td>
                    IEEE INFOCOM-2021 Student Grant Award

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%79%61%64.%78%69%65@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=bi8AlbMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://orcid.org/0000-0002-2467-4240" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://www.researchgate.net/profile/Yadong-Xie-2/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a>
            <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a>
            <div id="WeChatMod" class="wechat-modal">
                <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR">
            </div>
            

<!-- WeChat Modal -->

<script>
var wechatModal = document.getElementById("WeChatMod");
var wechatBtn = document.getElementById("WeChatBtn");

wechatBtn.onclick = function() {
  wechatModal.style.display = "block";
}

window.onclick = function(event) {
  if (event.target == wechatModal) {
    wechatModal.style.display = "none";
  }
}
</script>


            

              </div>

              <div class="contact-note">
                
              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Yadong  Xie. Last updated: August, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="/assets/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="/assets/js/mdb.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    

<!-- WeChat Modal -->

<script>
var wechatModal = document.getElementById("WeChatMod");
var wechatBtn = document.getElementById("WeChatBtn");

wechatBtn.onclick = function() {
  wechatModal.style.display = "block";
}

window.onclick = function(event) {
  if (event.target == wechatModal) {
    wechatModal.style.display = "none";
  }
}
</script>


  </body>
</html>
